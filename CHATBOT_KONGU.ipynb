{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTkvO9VitNZp8lYYi8pX96",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abissankar/ChatBot-KEC/blob/main/CHATBOT_KONGU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29dJYKPwqplx"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch datasets accelerate peft bitsandbytes trl\n",
        "!pip install langchain playwright html2text sentence_transformers faiss-gpu gradio\n",
        "!pip install langchain-community\n",
        "!playwright install chromium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
        "import torch\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "import nest_asyncio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "jwy2dyCvq_97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "3JBi_GKprSa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model and token details\n",
        "model_name = \"ministral/Ministral-3b-instruct\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZkJaCR-IrU6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()   # will ask for your new HF token\n"
      ],
      "metadata": {
        "id": "KtMWORo6tvRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model configuration\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_name, trust_remote_code=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "3RI6MQqdrWwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "JBhZnLlKraqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BitsAndBytes configuration\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)"
      ],
      "metadata": {
        "id": "xPYhKY9hrcot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")"
      ],
      "metadata": {
        "id": "YtGpz6gBre5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model with 4-bit precision\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "G3HHp7G6rioy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Hugging Face Transformers if not already\n",
        "!pip install transformers accelerate bitsandbytes -q\n",
        "\n",
        "#  Imports\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "f0iylGNMGrUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build pipeline\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    do_sample=False,         # deterministic output\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,  # âœ… only the model's answer\n",
        "    max_new_tokens=300,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "c6I0vufTrl7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup LangChain memory for conversation history\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\", return_messages=True)"
      ],
      "metadata": {
        "id": "mt9ZZwY6rm57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "prompt_template = \"\"\"\n",
        "### [INST]\n",
        "Instruction: Answer the question based on your\n",
        "Kongu Engineering College knowledge. Here is context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question}\n",
        "\n",
        "[/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "m2IZZj1Kro01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFacePipeline with the model\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "dZS4n8o7rtMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a prompt using the template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")"
      ],
      "metadata": {
        "id": "jf6SQ11fru7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LLMChain\n",
        "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt, memory=memory)"
      ],
      "metadata": {
        "id": "DPHW9RBgrxyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup embedding and retriever (for RAG)\n",
        "# Define the articles to scrape\n",
        "articles = [\n",
        "    \"https://en.wikipedia.org/wiki/Kongu_Engineering_College\",\n",
        "    \"https://www.kongu.edu/aboutus.html\"\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "p39K-cJYryjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape the articles for context\n",
        "!pip install playwright\n",
        "!playwright install\n",
        "\n",
        "loader = AsyncChromiumLoader(articles)\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "PzAJKJwxr17h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert HTML to plain text\n",
        "!pip install html2text\n",
        "\n",
        "html2text = Html2TextTransformer()\n",
        "docs_transformed = html2text.transform_documents(docs)"
      ],
      "metadata": {
        "id": "e6B9oJeHr4wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "id": "jMQRB8wqBQ-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk the text for indexing\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
        "# Load chunked documents into FAISS index\n",
        "db = FAISS.from_documents(chunked_documents, HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))"
      ],
      "metadata": {
        "id": "GkZvSiqVr5Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert FAISS into a retriever\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={'k': 4})"
      ],
      "metadata": {
        "id": "xbHVxWbUsVMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved chatbot interaction function\n",
        "def chatbot_interaction(context, question):\n",
        "    # Retrieve context if not provided\n",
        "    if not context.strip():\n",
        "        docs = retriever.get_relevant_documents(question)\n",
        "        context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Debug print to see what context is retrieved\n",
        "    print(\"DEBUG: Retrieved context:\\n\", context)\n",
        "\n",
        "    # More precise prompt to minimize hallucination\n",
        "    query = f\"\"\"\n",
        "Use ONLY the information in the context below to answer the question.\n",
        "If the answer is not contained in the context, respond with \"I don't know.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "    response = text_generation_pipeline(query)[0][\"generated_text\"]\n",
        "    return response.strip()\n"
      ],
      "metadata": {
        "id": "GPzKGpydFjvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio app\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot_interaction,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=5, label=\"Context (optional)\", placeholder=\"Paste context here or leave blank for auto-fetch\"),\n",
        "        gr.Textbox(lines=2, label=\"Question\", placeholder=\"Enter your question here\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Response\"),\n",
        "    title=\"Kongu Engineering College Chatbot\",\n",
        "    description=\"Ask questions about Kongu Engineering College. Provide context or let the chatbot retrieve information!\"\n",
        ")"
      ],
      "metadata": {
        "id": "dGpLdXDPsdFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch Gradio app\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "8n99h6rgshqo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}